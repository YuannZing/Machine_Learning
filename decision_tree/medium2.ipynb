{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def node_def(self) -> str:\n",
    "\n",
    "        if (self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])            \n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    Training: Use \"train\" function with train set features and labels\n",
    "    Predicting: Use \"predict\" function with test set features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, \n",
    "                 min_information_gain=0.0, numb_of_features_splitting=None,\n",
    "                 amount_of_say=None) -> None:\n",
    "        \"\"\"\n",
    "        Setting the class with hyperparameters\n",
    "        max_depth: (int) -> max depth of the tree\n",
    "        min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible\n",
    "        min_information_gain: (float) -> min information gain required to make the splitting possible\n",
    "        num_of_features_splitting: (str) ->  when splitting if sqrt then sqrt(# of features) features considered, \n",
    "                                                            if log then log(# of features) features considered\n",
    "                                                            else all features are considered\n",
    "        amount_of_say: (float) -> used for Adaboost algorithm                                                    \n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p>0])\n",
    "    \n",
    "    # def _class_probabilities(self, labels: list) -> list:\n",
    "    #     total_count = len(labels)\n",
    "    #     return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "    \n",
    "    def _class_probabilities(self, labels: np.array) -> list:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        return counts / counts.sum()\n",
    "\n",
    "\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1])\"\"\"\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    \n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        \n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    \n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        \"\"\"\n",
    "        Randomly selects the features to use while splitting w.r.t. hyperparameter numb_of_features_splitting\n",
    "        \"\"\"\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "        \n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"\n",
    "        Finds the best split (with the lowest entropy) given data\n",
    "        Returns 2 splitted groups and split information\n",
    "        \"\"\"\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use =  self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2, = self._split(data, idx, feature_val)\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    # def _find_label_probs(self, data: np.array) -> np.array:\n",
    "\n",
    "    #     labels_as_integers = data[:,-1].astype(int)\n",
    "    #     # Calculate the total number of labels\n",
    "    #     total_labels = len(labels_as_integers)\n",
    "    #     # Calculate the ratios (probabilities) for each label\n",
    "    #     label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "    #     # Populate the label_probabilities array based on the specific labels\n",
    "    #     for i, label in enumerate(self.labels_in_train):\n",
    "    #         label_index = np.where(labels_as_integers == i)[0]\n",
    "    #         if len(label_index) > 0:\n",
    "    #             label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "    #     return label_probabilities\n",
    "    \n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "        labels_as_integers = data[:, -1].astype(int)\n",
    "        unique, counts = np.unique(labels_as_integers, return_counts=True)\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        label_probabilities[unique] = counts / counts.sum()\n",
    "        return label_probabilities\n",
    "\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Recursive, depth first tree creation algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the max depth has been reached (stopping criteria)\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        # Find best split\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        \n",
    "        # Find label probs for the node\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "\n",
    "        # Calculate information gain\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        \n",
    "        # Create node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Returns prediction for 1 dim array\"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Finds the leaf which X belongs\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with given X and Y datasets\n",
    "        \"\"\"\n",
    "\n",
    "        # Concat features and labels\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        # Start creating the tree\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        # Normalize the feature importance values\n",
    "        self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted probs for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        \n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds    \n",
    "        \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node != None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
    "        if node != None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data iris.csv', delimiter=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat kamus untuk memetakan kategori ke angka\n",
    "class_mapping = {label: idx for idx, label in enumerate(df['class'].unique())}\n",
    "\n",
    "# Terapkan pemetaan ke kolom 'class'\n",
    "df['class'] = df['class'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1).values\n",
    "# y = df['class'].values.reshape(-1,1)\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, random_state=41, test_size=0.2):\n",
    "    # Get number of samples\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Set the seed for the random number generator\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Shuffle the indices\n",
    "    shuffled_indices = np.random.permutation(np.arange(n_samples))\n",
    "\n",
    "    # Determine the size of the test set\n",
    "    test_size = int(n_samples * test_size)\n",
    "\n",
    "    # Split the indices into test and train\n",
    "    test_indices = shuffled_indices[:test_size]\n",
    "    train_indices = shuffled_indices[test_size:]\n",
    "\n",
    "    # Split the features and target arrays into test and train\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (120, 4)\n",
      "Test Shape :  (30, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "print(\"Train Shape :\", X_train.shape)\n",
    "print(\"Test Shape : \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree(max_depth=4, min_samples_leaf=1)\n",
    "tree.train(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                -> LEAF | Label Counts = 0.0->1 | Pred Probs = [1. 0. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 4.4 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->8 | Pred Probs = [1. 0. 0.]\n",
      "                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 4.8 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->6 | Pred Probs = [1. 0. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.0 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->22 | Pred Probs = [1. 0. 0.]\n",
      "                -> NODE | Information Gain = 0.5991281323454871 | Split IF X[3] < 0.475 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->1 | Pred Probs = [1. 0. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.025 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->1 | Pred Probs = [1. 0. 0.]\n",
      "                                -> NODE | Information Gain = 0.6193821946787638 | Split IF X[3] < 1.0 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->3 | Pred Probs = [0. 1. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.3 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->8 | Pred Probs = [0. 1. 0.]\n",
      "-> NODE | Information Gain = 0.7091638062212156 | Split IF X[3] < 1.3 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->3 | Pred Probs = [0. 1. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.6 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->13 | Pred Probs = [0. 1. 0.]\n",
      "                                -> NODE | Information Gain = 0.17960871694752029 | Split IF X[3] < 1.5 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->7, 2.0->1 | Pred Probs = [0.    0.875 0.125]\n",
      "                                                -> NODE | Information Gain = 0.18254790807563637 | Split IF X[2] < 5.0 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->2, 2.0->3 | Pred Probs = [0.  0.4 0.6]\n",
      "                -> NODE | Information Gain = 0.6150833540894252 | Split IF X[3] < 1.8 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->1, 2.0->1 | Pred Probs = [0.  0.5 0.5]\n",
      "                                                -> NODE | Information Gain = 0.3166890883150208 | Split IF X[0] < 5.925000000000001 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->4 | Pred Probs = [0. 0. 1.]\n",
      "                                -> NODE | Information Gain = 0.0703018015599223 | Split IF X[2] < 5.1 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->6 | Pred Probs = [0. 0. 1.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 6.4 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->29 | Pred Probs = [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN PERFORMANCE\n",
      "Train Size 120\n",
      "True preds 116\n",
      "Train Accuracy 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Let's see the Train performance\n",
    "train_preds = tree.predict(X_set=X_train)\n",
    "print(\"TRAIN PERFORMANCE\")\n",
    "print(\"Train Size\", len(y_train))\n",
    "print(\"True preds\", sum(train_preds == y_train))\n",
    "print(\"Train Accuracy\", sum(train_preds == y_train) / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PERFORMANCE\n",
      "Test Size 30\n",
      "True preds 28\n",
      "Test Accuracy 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Let's see the Test performance\n",
    "test_preds = tree.predict(X_set=X_test)\n",
    "print(\"TEST PERFORMANCE\")\n",
    "print(\"Test Size\", len(y_test))\n",
    "print(\"True preds\", sum(test_preds == y_test))\n",
    "print(\"Test Accuracy\", sum(test_preds == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  2  4]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, labels=None):\n",
    "    \"\"\"\n",
    "    Membuat confusion matrix dari label sebenarnya dan prediksi.\n",
    "    :param y_true: List atau array label sebenarnya\n",
    "    :param y_pred: List atau array label prediksi\n",
    "    :param labels: List dari semua label unik (opsional)\n",
    "    :return: Confusion matrix sebagai numpy array\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    \n",
    "    matrix = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "    label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[label_to_index[true], label_to_index[pred]] += 1\n",
    "    \n",
    "    return matrix, labels\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    \"\"\"\n",
    "    Memvisualisasikan confusion matrix.\n",
    "    :param cm: Confusion matrix dalam bentuk numpy array\n",
    "    :param labels: Label kelas\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "cm, labels = confusion_matrix(y_test, test_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "plot_confusion_matrix(cm, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
