{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def node_def(self) -> str:\n",
    "\n",
    "        if (self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])            \n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    Training: Use \"train\" function with train set features and labels\n",
    "    Predicting: Use \"predict\" function with test set features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, \n",
    "                 min_information_gain=0.0, numb_of_features_splitting=None,\n",
    "                 amount_of_say=None) -> None:\n",
    "        \"\"\"\n",
    "        Setting the class with hyperparameters\n",
    "        max_depth: (int) -> max depth of the tree\n",
    "        min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible\n",
    "        min_information_gain: (float) -> min information gain required to make the splitting possible\n",
    "        num_of_features_splitting: (str) ->  when splitting if sqrt then sqrt(# of features) features considered, \n",
    "                                                            if log then log(# of features) features considered\n",
    "                                                            else all features are considered\n",
    "        amount_of_say: (float) -> used for Adaboost algorithm                                                    \n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p>0])\n",
    "    \n",
    "    # def _class_probabilities(self, labels: list) -> list:\n",
    "    #     total_count = len(labels)\n",
    "    #     return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "    \n",
    "    def _class_probabilities(self, labels: np.array) -> list:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        return counts / counts.sum()\n",
    "\n",
    "\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1])\"\"\"\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    \n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        \n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    \n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        \"\"\"\n",
    "        Randomly selects the features to use while splitting w.r.t. hyperparameter numb_of_features_splitting\n",
    "        \"\"\"\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "        \n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"\n",
    "        Finds the best split (with the lowest entropy) given data\n",
    "        Returns 2 splitted groups and split information\n",
    "        \"\"\"\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use =  self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2, = self._split(data, idx, feature_val)\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    # def _find_label_probs(self, data: np.array) -> np.array:\n",
    "\n",
    "    #     labels_as_integers = data[:,-1].astype(int)\n",
    "    #     # Calculate the total number of labels\n",
    "    #     total_labels = len(labels_as_integers)\n",
    "    #     # Calculate the ratios (probabilities) for each label\n",
    "    #     label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "    #     # Populate the label_probabilities array based on the specific labels\n",
    "    #     for i, label in enumerate(self.labels_in_train):\n",
    "    #         label_index = np.where(labels_as_integers == i)[0]\n",
    "    #         if len(label_index) > 0:\n",
    "    #             label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "    #     return label_probabilities\n",
    "    \n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "        labels_as_integers = data[:, -1].astype(int)\n",
    "        unique, counts = np.unique(labels_as_integers, return_counts=True)\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        label_probabilities[unique] = counts / counts.sum()\n",
    "        return label_probabilities\n",
    "\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Recursive, depth first tree creation algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the max depth has been reached (stopping criteria)\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        # Find best split\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        \n",
    "        # Find label probs for the node\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "\n",
    "        # Calculate information gain\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        \n",
    "        # Create node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Returns prediction for 1 dim array\"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Finds the leaf which X belongs\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with given X and Y datasets\n",
    "        \"\"\"\n",
    "\n",
    "        # Concat features and labels\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        # Start creating the tree\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        # Normalize the feature importance values\n",
    "        self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted probs for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        \n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds    \n",
    "        \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node != None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
    "        if node != None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (112, 4)\n",
      "Test Shape :  (38, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = np.array(iris.data)\n",
    "y = np.array(iris.target)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "print(\"Train Shape :\", X_train.shape)\n",
    "print(\"Test Shape : \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree(max_depth=4, min_samples_leaf=1)\n",
    "tree.train(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                -> LEAF | Label Counts = 0.0->1 | Pred Probs = [1. 0. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 4.4 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->8 | Pred Probs = [1. 0. 0.]\n",
      "                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 4.775 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->6 | Pred Probs = [1. 0. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.0 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->21 | Pred Probs = [1. 0. 0.]\n",
      "                -> NODE | Information Gain = 0.6731015728685515 | Split IF X[3] < 0.525 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 0.0->1 | Pred Probs = [1. 0. 0.]\n",
      "                                -> NODE | Information Gain = 0.41381685030363374 | Split IF X[3] < 1.0 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->3 | Pred Probs = [0. 1. 0.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.3 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->8 | Pred Probs = [0. 1. 0.]\n",
      "-> NODE | Information Gain = 0.7095311377468411 | Split IF X[3] < 1.3 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->2, 2.0->1 | Pred Probs = [0.         0.66666667 0.33333333]\n",
      "                                                -> NODE | Information Gain = 0.14865258200778284 | Split IF X[0] < 5.6 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->17 | Pred Probs = [0. 1. 0.]\n",
      "                                -> NODE | Information Gain = 0.23041810979993893 | Split IF X[2] < 4.8 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->1, 2.0->4 | Pred Probs = [0.  0.2 0.8]\n",
      "                                                -> NODE | Information Gain = 0.22943684069673975 | Split IF X[1] < 3.0 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->3, 2.0->1 | Pred Probs = [0.   0.75 0.25]\n",
      "                -> NODE | Information Gain = 0.6088858430669675 | Split IF X[2] < 5.1 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->2 | Pred Probs = [0. 0. 1.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.85 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->5 | Pred Probs = [0. 0. 1.]\n",
      "                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 6.4 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->7 | Pred Probs = [0. 0. 1.]\n",
      "                                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 6.65 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->21 | Pred Probs = [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN PERFORMANCE\n",
      "Train Size 112\n",
      "True preds 109\n",
      "Train Accuracy 0.9732142857142857\n"
     ]
    }
   ],
   "source": [
    "# Let's see the Train performance\n",
    "train_preds = tree.predict(X_set=X_train)\n",
    "print(\"TRAIN PERFORMANCE\")\n",
    "print(\"Train Size\", len(y_train))\n",
    "print(\"True preds\", sum(train_preds == y_train))\n",
    "print(\"Train Accuracy\", sum(train_preds == y_train) / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PERFORMANCE\n",
      "Test Size 38\n",
      "True preds 35\n",
      "Test Accuracy 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# Let's see the Test performance\n",
    "test_preds = tree.predict(X_set=X_test)\n",
    "print(\"TEST PERFORMANCE\")\n",
    "print(\"Test Size\", len(y_test))\n",
    "print(\"True preds\", sum(test_preds == y_test))\n",
    "print(\"Test Accuracy\", sum(test_preds == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (455, 30)\n",
      "Test Shape :  (114, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(\"Train Shape :\", X_train.shape)\n",
    "print(\"Test Shape : \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the tree\n",
    "tree_2 = DecisionTree(max_depth=4, min_samples_leaf=1, min_information_gain=0.05)\n",
    "tree_2.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                -> LEAF | Label Counts = 0.0->6, 1.0->249 | Pred Probs = [0.02352941 0.97647059]\n",
      "                -> NODE | Information Gain = 0.24446648320004927 | Split IF X[27] < 0.112 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->16 | Pred Probs = [0. 1.]\n",
      "                                                -> NODE | Information Gain = 0.20443400292496505 | Split IF X[26] < 0.24924999999999997 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->24, 1.0->24 | Pred Probs = [0.5 0.5]\n",
      "                                -> NODE | Information Gain = 0.28620952259378585 | Split IF X[27] < 0.177425 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 0.0->22 | Pred Probs = [1. 0.]\n",
      "-> NODE | Information Gain = 0.4649825668927295 | Split IF X[20] < 18.409999999999997 THEN left O/W right\n",
      "                -> LEAF | Label Counts = 0.0->113, 1.0->1 | Pred Probs = [0.99122807 0.00877193]\n"
     ]
    }
   ],
   "source": [
    "# Visualisasi pohon keputusan\n",
    "tree_2.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN PERFORMANCE\n",
      "Train size 455\n",
      "True preds 424\n",
      "Train Accuracy 0.9318681318681319\n"
     ]
    }
   ],
   "source": [
    "# Let's see the Train performance\n",
    "train_preds = tree_2.predict(X_set=X_train)\n",
    "print(\"TRAIN PERFORMANCE\")\n",
    "print(\"Train size\", len(y_train))\n",
    "print(\"True preds\", sum(train_preds == y_train))\n",
    "print(\"Train Accuracy\", sum(train_preds == y_train) / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PERFORMANCE\n",
      "Test size 114\n",
      "True preds 103\n",
      "Accuracy 0.9035087719298246\n"
     ]
    }
   ],
   "source": [
    "# Let's see the Test performance\n",
    "test_preds = tree_2.predict(X_set=X_test)\n",
    "print(\"TEST PERFORMANCE\")\n",
    "print(\"Test size\", len(y_test))\n",
    "print(\"True preds\", sum(test_preds == y_test))\n",
    "print(\"Accuracy\", sum(test_preds == y_test) / len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
