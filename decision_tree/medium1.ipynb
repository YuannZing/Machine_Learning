{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def node_def(self) -> str:\n",
    "\n",
    "        if (self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])            \n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    Training: Use \"train\" function with train set features and labels\n",
    "    Predicting: Use \"predict\" function with test set features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, \n",
    "                 min_information_gain=0.0, numb_of_features_splitting=None,\n",
    "                 amount_of_say=None) -> None:\n",
    "        \"\"\"\n",
    "        Setting the class with hyperparameters\n",
    "        max_depth: (int) -> max depth of the tree\n",
    "        min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible\n",
    "        min_information_gain: (float) -> min information gain required to make the splitting possible\n",
    "        num_of_features_splitting: (str) ->  when splitting if sqrt then sqrt(# of features) features considered, \n",
    "                                                            if log then log(# of features) features considered\n",
    "                                                            else all features are considered\n",
    "        amount_of_say: (float) -> used for Adaboost algorithm                                                    \n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p>0])\n",
    "    \n",
    "    # def _class_probabilities(self, labels: list) -> list:\n",
    "    #     total_count = len(labels)\n",
    "    #     return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "    \n",
    "    def _class_probabilities(self, labels: np.array) -> list:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        return counts / counts.sum()\n",
    "\n",
    "\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1])\"\"\"\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    \n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        \n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    \n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        \"\"\"\n",
    "        Randomly selects the features to use while splitting w.r.t. hyperparameter numb_of_features_splitting\n",
    "        \"\"\"\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "        \n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"\n",
    "        Finds the best split (with the lowest entropy) given data\n",
    "        Returns 2 splitted groups and split information\n",
    "        \"\"\"\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use =  self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2, = self._split(data, idx, feature_val)\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    # def _find_label_probs(self, data: np.array) -> np.array:\n",
    "\n",
    "    #     labels_as_integers = data[:,-1].astype(int)\n",
    "    #     # Calculate the total number of labels\n",
    "    #     total_labels = len(labels_as_integers)\n",
    "    #     # Calculate the ratios (probabilities) for each label\n",
    "    #     label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "    #     # Populate the label_probabilities array based on the specific labels\n",
    "    #     for i, label in enumerate(self.labels_in_train):\n",
    "    #         label_index = np.where(labels_as_integers == i)[0]\n",
    "    #         if len(label_index) > 0:\n",
    "    #             label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "    #     return label_probabilities\n",
    "    \n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "        labels_as_integers = data[:, -1].astype(int)\n",
    "        unique, counts = np.unique(labels_as_integers, return_counts=True)\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        label_probabilities[unique] = counts / counts.sum()\n",
    "        return label_probabilities\n",
    "\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Recursive, depth first tree creation algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the max depth has been reached (stopping criteria)\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        # Find best split\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        \n",
    "        # Find label probs for the node\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "\n",
    "        # Calculate information gain\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        \n",
    "        # Create node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Returns prediction for 1 dim array\"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Finds the leaf which X belongs\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with given X and Y datasets\n",
    "        \"\"\"\n",
    "\n",
    "        # Concat features and labels\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        # Start creating the tree\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        # Normalize the feature importance values\n",
    "        self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted probs for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        \n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds    \n",
    "        \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node != None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
    "        if node != None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh dataset sederhana (X: fitur, y: label)\n",
    "X_train = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],\n",
    "    [4.9, 3.0, 1.4, 0.2],\n",
    "    [6.2, 2.9, 4.3, 1.3],\n",
    "    [5.8, 2.7, 4.1, 1.0],\n",
    "    [6.5, 3.0, 5.8, 2.2],\n",
    "    [7.1, 3.0, 5.9, 2.1]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 0, 1, 1, 2, 2])  # Label kelas\n",
    "\n",
    "# Jika dataset dalam bentuk pandas DataFrame\n",
    "df = pd.DataFrame(X_train, columns=[\"F1\", \"F2\", \"F3\", \"F4\"])\n",
    "df[\"Label\"] = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F1   F2   F3   F4  Label\n",
       "0  5.1  3.5  1.4  0.2      0\n",
       "1  4.9  3.0  1.4  0.2      0\n",
       "2  6.2  2.9  4.3  1.3      1\n",
       "3  5.8  2.7  4.1  1.0      1\n",
       "4  6.5  3.0  5.8  2.2      2\n",
       "5  7.1  3.0  5.9  2.1      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil prediksi: [1]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([[6.0, 3.0, 4.5, 1.5]])  # Contoh input data\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "print(\"Hasil prediksi:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                -> LEAF | Label Counts = 0.0->1 | Pred Probs = [1. 0. 0.]\n",
      "                -> NODE | Information Gain = 0.0 | Split IF X[0] < 4.95 THEN left O/W right\n",
      "                                -> LEAF | Label Counts = 0.0->1 | Pred Probs = [1. 0. 0.]\n",
      "-> NODE | Information Gain = 0.9182958340544894 | Split IF X[0] < 5.2749999999999995 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 1.0->1 | Pred Probs = [0. 1. 0.]\n",
      "                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 5.9 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 1.0->1 | Pred Probs = [0. 1. 0.]\n",
      "                -> NODE | Information Gain = 1.0 | Split IF X[0] < 6.35 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 2.0->1 | Pred Probs = [0. 0. 1.]\n",
      "                                -> NODE | Information Gain = 0.0 | Split IF X[0] < 6.65 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 2.0->1 | Pred Probs = [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Visualisasi pohon keputusan\n",
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pohon = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil prediksi: [1]\n"
     ]
    }
   ],
   "source": [
    "pohon.fit(X_train, y_train)\n",
    "X_test = np.array([[6.0, 3.0, 4.5, 1.5]])  # Contoh input data\n",
    "\n",
    "y_pred = pohon.predict(X_test)\n",
    "print(\"Hasil prediksi:\", y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
