{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs,information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "    def node_leaf(self)->str:\n",
    "        if(self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} | Then Left O/W Right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])\n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, numb_of_feature_splitting=None, amount_of_say=None) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_the_splitting = numb_of_feature_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilties: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilties if p > 0])\n",
    "\n",
    "    def _class_probabilities(self, labels: np.array) -> list:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        return counts / counts.sum()\n",
    "\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "\n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "\n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "\n",
    "    def _select_feature_to_use(self, data: np.array) -> list:\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_the_splitting == \"sqrt\":\n",
    "            feature_idx_to_user = np.random.choice(\n",
    "                feature_idx, size=int(np.sqrt(len(feature_idx)))\n",
    "            )\n",
    "        elif self.numb_of_the_splitting == \"log\":\n",
    "            feature_idx_to_user = np.random.choice(\n",
    "                feature_idx, size=int(np.log2(len(feature_idx)))\n",
    "            )\n",
    "        else:\n",
    "            feature_idx_to_user = feature_idx\n",
    "        return feature_idx_to_user\n",
    "\n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use = self._select_feature_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(\n",
    "                data[:, idx], q=np.arange(25, 100, 25))\n",
    "\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2 = self._split(data, idx, feature_val)\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    def _find_labels_probs(self, data: np.array) -> np.array:\n",
    "        labels_as_intergers = data[:, -1].astype(int)\n",
    "        unique, counts = np.unique(labels_as_intergers, return_counts=True)\n",
    "        label_probability = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        label_probability[unique] = counts / counts.sum()\n",
    "\n",
    "        return label_probability\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_vals, split_entropy = self._find_best_split(\n",
    "            data)\n",
    "\n",
    "        labels_probabilities = self._find_labels_probs(data)\n",
    "\n",
    "        node_entropy = self._entropy(labels_probabilities)\n",
    "\n",
    "        information_gain = node_entropy - split_entropy\n",
    "\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_vals,\n",
    "                        labels_probabilities, information_gain)\n",
    "\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        return node\n",
    "\n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        node = self.tree\n",
    "\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate(\n",
    "            (X_train, np.reshape(Y_train, (-1, 1))), axis=1\n",
    "        )\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        self.feature_importances = {k: v / total for total in (sum(\n",
    "            self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
    "        \n",
    "    def predict_proba(self, X_set: np.array)-> np.array:\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        return pred_probs\n",
    "    \n",
    "    def predict(self, X_set:np.array)-> np.array:\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node != None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_leaf())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "            \n",
    "    def print_tree(self)->None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "        \n",
    "    def _calculate_feature_importance(self, node):\n",
    "        if node != None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, random_state=42, test_size=0.2):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    shuffled_indices = np.random.permutation(np.arange(n_samples))\n",
    "    \n",
    "    test_size = int(n_samples * test_size)\n",
    "    \n",
    "    test_indices = shuffled_indices[:test_size]\n",
    "    train_indices = shuffled_indices[test_size:]\n",
    "    \n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
